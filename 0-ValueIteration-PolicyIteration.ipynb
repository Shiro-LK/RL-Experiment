{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : \n",
    "We have two important functions : \n",
    "- $V^\\pi(s)$ is the state-value function of MDP (Markov Decision Process). It's the expected return starting from state s following policy $\\pi$. \n",
    "- $Q^\\pi(s)$ is the action-value function. It is the expected return starting from state s, following policy $\\pi$, taking action $a$\n",
    "\n",
    "The Bellman Equations are : \n",
    "\n",
    "$V_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(R_{s}^{a}+\\sum_{s^{\\prime} \\in S} \\gamma \\mathcal{P}_{s s^{\\prime}}^{a} V_{\\pi}\\left(s^{\\prime}\\right)\\right)$ with $R$ the reward, $\\gamma$ the discount, $P$ the transition matrix probability, $V$ the value function, $s$ the state, $a$ the action and $\\pi$ the probability of making the decision in state $s$ (policy).\n",
    "\n",
    "$q_{\\pi}(s, a)=R_{s}^{a}+\\sum_{s^{\\prime} \\in \\mathcal{S}} \\gamma \\mathcal{P}_{s s^{\\prime}}^{a} \\underset{a \\in \\mathcal{A}}{\\pi\\left(a \\mid s^{\\prime}\\right)} q_{\\pi}\\left(s^{\\prime}, a\\right)$ with $a$ the action, and $P$  the probability that action $a$ in state $s$ time t will lead to state $s′$ at time $t + 1$\n",
    "\n",
    "Usually, we are interested in the optimal state function or action-state function in order to find then the best policy :\n",
    "\n",
    "\\begin{equation*}V^*_{\\pi}(s)=\\sum_{max  a} \\pi(a \\mid s)\\left(R_{s}^{a}+\\sum_{s^{\\prime} \\in S} \\gamma \\mathcal{P}_{s s^{\\prime}}^{a} V_{\\pi}\\left(s^{\\prime}\\right)\\right)\\end{equation*} \\\\\n",
    "\\begin{equation*}\n",
    "q^*_{\\pi}(s, a)=R_{s}^{a}+ max_{a\\prime} \\sum_{s^{\\prime} \\in \\mathcal{S}} \\gamma \\mathcal{P}_{s s^{\\prime}}^{a} \\underset{a \\in \\mathcal{A}}{\\pi\\left(a\\prime \\mid s^{\\prime}\\right)} q_{\\pi}\\left(s^{\\prime}, a\\prime\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Then we can extract the policy such as : \n",
    "\n",
    "\\begin{equation*}\\pi^* = argmax_a \\left(R_{s}^{a}+\\sum_{s^{\\prime} \\in S} \\gamma \\mathcal{P}_{s s^{\\prime}}^{a} V_{\\pi}\\left(s^{\\prime}\\right)\\right)\\end{equation*} \n",
    "\n",
    "In this notebook, we will reproduce two dynamic programming algorithm which allow to find the optimal policy for a known environment. Our hypothesis are : \n",
    "- no move outside of the map (square of size 5x5)\n",
    "- the coordinate (5,5) are the final destination, and the reward is equal to 1. The reward of the other positions are 0.\n",
    "<img src=\"img/map.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "<img src=\"img/value_iteration.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP : \n",
      " [[ 0. nan  0. nan  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0. nan nan  0.  0.]\n",
      " [ 0. nan  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "REWARD : \n",
      " [[ 0. nan  0. nan  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0. nan nan  0.  0.]\n",
      " [ 0. nan  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "import numpy as np\n",
    "\n",
    "table = np.zeros((5,5))\n",
    "table[0, 1] = np.nan\n",
    "table[0, 3] = np.nan\n",
    "table[2, 1] = np.nan\n",
    "table[3, 1] = np.nan\n",
    "table[2,2] = np.nan\n",
    "print(\"MAP : \\n\", table)\n",
    "R = table.copy()\n",
    "R[-1,-1] = 1.0\n",
    "print(\"REWARD : \\n\", R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 0\n",
      "[[ 0. nan  0. nan  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0. nan nan  0.  0.]\n",
      " [ 0. nan  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "K = 1\n",
      "[[0.  nan 0.  nan 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  nan nan 0.  0. ]\n",
      " [0.  nan 0.  0.  0.9]\n",
      " [0.  0.  0.  0.9 1. ]]\n",
      "K = 2\n",
      "[[0.    nan 0.    nan 0.  ]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.    nan  nan 0.   0.81]\n",
      " [0.    nan 0.   0.81 0.9 ]\n",
      " [0.   0.   0.81 0.9  1.  ]]\n",
      "K = 3\n",
      "[[0.      nan 0.      nan 0.   ]\n",
      " [0.    0.    0.    0.    0.729]\n",
      " [0.      nan   nan 0.729 0.81 ]\n",
      " [0.      nan 0.729 0.81  0.9  ]\n",
      " [0.    0.729 0.81  0.9   1.   ]]\n",
      "K = 4\n",
      "[[0.        nan 0.        nan 0.6561]\n",
      " [0.     0.     0.     0.6561 0.729 ]\n",
      " [0.        nan    nan 0.729  0.81  ]\n",
      " [0.        nan 0.729  0.81   0.9   ]\n",
      " [0.6561 0.729  0.81   0.9    1.    ]]\n",
      "K = 5\n",
      "[[0.          nan 0.          nan 0.6561 ]\n",
      " [0.      0.      0.59049 0.6561  0.729  ]\n",
      " [0.          nan     nan 0.729   0.81   ]\n",
      " [0.59049     nan 0.729   0.81    0.9    ]\n",
      " [0.6561  0.729   0.81    0.9     1.     ]]\n",
      "K = 6\n",
      "[[0.            nan 0.531441      nan 0.6561  ]\n",
      " [0.       0.531441 0.59049  0.6561   0.729   ]\n",
      " [0.531441      nan      nan 0.729    0.81    ]\n",
      " [0.59049       nan 0.729    0.81     0.9     ]\n",
      " [0.6561   0.729    0.81     0.9      1.      ]]\n",
      "K = 7\n",
      "[[0.              nan 0.531441        nan 0.6561   ]\n",
      " [0.4782969 0.531441  0.59049   0.6561    0.729    ]\n",
      " [0.531441        nan       nan 0.729     0.81     ]\n",
      " [0.59049         nan 0.729     0.81      0.9      ]\n",
      " [0.6561    0.729     0.81      0.9       1.       ]]\n",
      "K = 8\n",
      "[[0.43046721        nan 0.531441          nan 0.6561    ]\n",
      " [0.4782969  0.531441   0.59049    0.6561     0.729     ]\n",
      " [0.531441          nan        nan 0.729      0.81      ]\n",
      " [0.59049           nan 0.729      0.81       0.9       ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      "K = 9\n",
      "[[0.43046721        nan 0.531441          nan 0.6561    ]\n",
      " [0.4782969  0.531441   0.59049    0.6561     0.729     ]\n",
      " [0.531441          nan        nan 0.729      0.81      ]\n",
      " [0.59049           nan 0.729      0.81       0.9       ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      "POLICY \n",
      " [['↓' nan '↓' nan '↓']\n",
      " ['→' '→' '→' '→' '↓']\n",
      " ['↓' nan nan '→' '↓']\n",
      " ['↓' nan '→' '→' '↓']\n",
      " ['→' '→' '→' '→' 0]]\n"
     ]
    }
   ],
   "source": [
    "def compute_max_state(V, R, discount, i, j):\n",
    "    max_ = []\n",
    "    max_i = V.shape[0]\n",
    "    max_j = V.shape[1]\n",
    "    # top\n",
    "    if i-1>=0 and not np.isnan(V[i-1,j]):\n",
    "        max_.append(R[i,j] + discount*V[i-1,j])\n",
    "    else:\n",
    "        max_.append(float(\"-inf\"))\n",
    "    \n",
    "    # right\n",
    "    if j+1<max_j and not np.isnan(V[i,j+1]):\n",
    "        max_.append(R[i,j] + discount*V[i,j+1])\n",
    "    else:\n",
    "        max_.append(float(\"-inf\"))\n",
    "        \n",
    "    # bot\n",
    "    if i+1<max_i and not np.isnan(V[i+1,j]):\n",
    "        max_.append(R[i,j] + discount*V[i+1,j])\n",
    "    else:\n",
    "        max_.append(float(\"-inf\"))    \n",
    "    # left\n",
    "    if j-1>=0 and not np.isnan(V[i,j-1]):\n",
    "        max_.append(R[i,j] + discount*V[i,j-1])\n",
    "    else:\n",
    "        max_.append(float(\"-inf\"))\n",
    "        \n",
    "    return max_\n",
    "\n",
    "def run_VI(table, R, discount=0.9, epsilon=0.01): # Value Iteration algo\n",
    "    V = table.copy()\n",
    "    temp = table.copy()\n",
    "    k = 0\n",
    "    while np.abs(np.nan_to_num(V - temp)).sum() > epsilon or k == 0:\n",
    "        # look every state\n",
    "        V = temp.copy()\n",
    "        temp = table.copy()\n",
    "        for i in range(table.shape[0]):\n",
    "            for j in range(table.shape[1]):\n",
    "                if not np.isnan(V[i,j]):\n",
    "                    if i == table.shape[0]-1 and j == table.shape[1]-1:\n",
    "                        temp[i,j] = R[i,j]\n",
    "                    else:\n",
    "                        temp[i,j] = max(compute_max_state(V, R, discount,i,j))\n",
    "                \n",
    "        print(f'K = {k}')\n",
    "        print(temp)\n",
    "        \n",
    "        k += 1\n",
    "    return temp\n",
    "def extract_policy(V, R, policy, discount):\n",
    "    mapping = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\"}\n",
    "    \n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            if not np.isnan(V[i,j]):\n",
    "                if i == table.shape[0]-1 and j == table.shape[1]-1:\n",
    "                    policy[i,j] = 0\n",
    "                else:\n",
    "                    policy[i,j] = mapping[np.argmax(compute_max_state(V, R, discount, i, j))]\n",
    "    return policy\n",
    "V = run_VI(table, R, discount=0.9, epsilon=0.01)\n",
    "policy = extract_policy(V, R, table.copy().astype(object), discount=0.9)    \n",
    "print('POLICY \\n',policy)            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy seems to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "<img src=\"img/Policy_iteration.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "politique = table.copy()\n",
    "politique = np.nan_to_num(politique, nan=-1).astype(object) # 0 top, 1 right, 2 = bot , 3 = left\n",
    "mapping = {\"↑\":0, \"→\":1, \"↓\":2, \"←\":3}\n",
    "\n",
    "politique[:-1, 0] = \"↓\"\n",
    "politique[0,2] = \"↓\"\n",
    "politique[0,4] = \"↓\"\n",
    "politique[1, 3] = \"↓\"\n",
    "politique[3,3] = \"↓\"\n",
    "politique[1:-1, -1] = \"↑\"\n",
    "politique[-1,:-1] = \"→\"\n",
    "politique[3,2] = \"→\"\n",
    "politique[1, 1:3] = \"→\"\n",
    "politique[2,3] = \"→\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_value(V, R, policy, discount, mapping, i, j): # the policy is determinism in this exercise\n",
    "    l = 0\n",
    "    h = 0\n",
    "    \n",
    "    if policy[i,j] == 0:\n",
    "        return R[i,j]\n",
    "    \n",
    "    if  mapping[policy[i,j]] == 0:\n",
    "        h = -1\n",
    "    elif mapping[policy[i,j]] == 1:\n",
    "        l = 1\n",
    "    elif mapping[policy[i,j]] == 2:\n",
    "        h = 1\n",
    "    elif mapping[policy[i,j]] == 3:\n",
    "        l = -1\n",
    "    \n",
    "    return R[i,j] + discount * V[i+h, j+l]\n",
    "\n",
    "def policy_evaluation(V, R, policy, mapping, discount, epsilon=0.1):\n",
    "    temp = V.copy()\n",
    "    \n",
    "    k = 0\n",
    "    delta = 0\n",
    "    while delta > epsilon or k ==0:\n",
    "        delta = 0\n",
    "        V = temp.copy()\n",
    "        #temp = V.copy()\n",
    "        for i in range(V.shape[0]):\n",
    "            for j in range(V.shape[1]):\n",
    "                if not np.isnan(V[i,j]):\n",
    "                    temp[i,j] = compute_value(V, R, policy, discount, mapping, i, j)\n",
    "        #print(temp)\n",
    "                    delta = max(delta, np.abs(np.nan_to_num(V[i,j]-temp[i,j])))\n",
    "        k +=1\n",
    "        #print(delta)\n",
    "    V = temp.copy()\n",
    "    return V\n",
    "\n",
    "def compute_best_action(V, R, discount, i, j, previous_policy):\n",
    "    max_ = []\n",
    "    max_i = V.shape[0]\n",
    "    max_j = V.shape[1]\n",
    "    # top\n",
    "    if i-1>=0 and not np.isnan(V[i-1,j]):\n",
    "        max_.append(R[i,j] + discount*V[i-1,j])\n",
    "    else:\n",
    "        max_.append(-float(\"inf\"))\n",
    "    # right\n",
    "    if j+1<max_j and not np.isnan(V[i,j+1]):\n",
    "        max_.append(R[i,j] + discount*V[i,j+1])\n",
    "    else:\n",
    "        max_.append(-float(\"inf\"))\n",
    "    # bot\n",
    "    if i+1<max_i and not np.isnan(V[i+1,j]):\n",
    "        max_.append(R[i,j] + discount*V[i+1,j])\n",
    "    else:\n",
    "        max_.append(-float(\"inf\"))    \n",
    "    # left\n",
    "    if j-1>=0 and not np.isnan(V[i,j-1]):\n",
    "        max_.append(R[i,j] + discount*V[i,j-1])\n",
    "    else:\n",
    "        max_.append(-float(\"inf\"))\n",
    "        \n",
    "    # if multiple argmax, and if previous policy is part of the argmax return it\n",
    "    argmaxes = np.argwhere(max_ == np.amax(max_)).flatten()\n",
    "    #print(i,j, max_)\n",
    "    #print(argmaxes)\n",
    "    for idx in argmaxes:\n",
    "        if idx == previous_policy:\n",
    "            return idx\n",
    "    \n",
    "    return np.argmax(np.array(max_))\n",
    "\n",
    "def policy_improvement(V, R, policy, mapping, discount):\n",
    "    reverse = {v:k for k,v in mapping.items()} # int to arrow\n",
    "    new = policy.copy()\n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            if (i != V.shape[0]-1 or j != V.shape[1]-1) and not np.isnan(V[i,j]):\n",
    "                new[i,j] = reverse[compute_best_action(V,R, discount=discount, i=i, j=j, \n",
    "                                                       previous_policy=mapping[policy[i,j]]) ]\n",
    "    return new\n",
    "\n",
    "\n",
    "def PolicyIteration(V,R,policy,mapping,discount,epsilon):\n",
    "    new_policy = policy.copy()\n",
    "    \n",
    "    k = 0\n",
    "    while k == 0 or (new_policy==policy).all() == False:\n",
    "        policy = new_policy.copy()\n",
    "        \n",
    "        # evaluate policy\n",
    "        V = policy_evaluation(V, R, policy, mapping, discount, epsilon=epsilon)\n",
    "        # improve policy\n",
    "        new_policy = policy_improvement(V, R, policy, mapping, discount)\n",
    "        print(f\" K = {k}\")\n",
    "        print(new_policy)\n",
    "        print(V)\n",
    "        k+=1\n",
    "    return V, new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['↓', -1.0, '↓', -1.0, '↓'],\n",
       "        ['↓', '→', '→', '↓', '↑'],\n",
       "        ['↓', -1.0, -1.0, '→', '↑'],\n",
       "        ['↓', -1.0, '→', '↓', '↑'],\n",
       "        ['→', '→', '→', '→', 0.0]], dtype=object),\n",
       " array([[ 0., nan,  0., nan,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan,  0.,  0.],\n",
       "        [ 0., nan,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politique, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K = 0\n",
      "[['↓' -1.0 '↓' -1.0 '↓']\n",
      " ['↓' '←' '→' '↓' '↑']\n",
      " ['↓' -1.0 -1.0 '↓' '↑']\n",
      " ['↓' -1.0 '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' 0.0]]\n",
      "[[0.43046721        nan 0.                nan 0.        ]\n",
      " [0.4782969  0.         0.         0.         0.        ]\n",
      " [0.531441          nan        nan 0.         0.        ]\n",
      " [0.59049           nan 0.729      0.81       0.        ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      " K = 1\n",
      "[['↓' -1.0 '↓' -1.0 '↓']\n",
      " ['↓' '→' '→' '↓' '←']\n",
      " ['↓' -1.0 -1.0 '↓' '↓']\n",
      " ['↓' -1.0 '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' 0.0]]\n",
      "[[0.43046721        nan 0.531441          nan 0.        ]\n",
      " [0.4782969  0.43046721 0.59049    0.6561     0.        ]\n",
      " [0.531441          nan        nan 0.729      0.        ]\n",
      " [0.59049           nan 0.729      0.81       0.9       ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      " K = 2\n",
      "[['↓' -1.0 '↓' -1.0 '↓']\n",
      " ['↓' '→' '→' '↓' '↓']\n",
      " ['↓' -1.0 -1.0 '↓' '↓']\n",
      " ['↓' -1.0 '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' 0.0]]\n",
      "[[0.43046721        nan 0.531441          nan 0.531441  ]\n",
      " [0.4782969  0.531441   0.59049    0.6561     0.59049   ]\n",
      " [0.531441          nan        nan 0.729      0.81      ]\n",
      " [0.59049           nan 0.729      0.81       0.9       ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      " K = 3\n",
      "[['↓' -1.0 '↓' -1.0 '↓']\n",
      " ['↓' '→' '→' '↓' '↓']\n",
      " ['↓' -1.0 -1.0 '↓' '↓']\n",
      " ['↓' -1.0 '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' 0.0]]\n",
      "[[0.43046721        nan 0.531441          nan 0.6561    ]\n",
      " [0.4782969  0.531441   0.59049    0.6561     0.729     ]\n",
      " [0.531441          nan        nan 0.729      0.81      ]\n",
      " [0.59049           nan 0.729      0.81       0.9       ]\n",
      " [0.6561     0.729      0.81       0.9        1.        ]]\n",
      "Policy of POLICY ITERATION :\n",
      "  [['↓' -1.0 '↓' -1.0 '↓']\n",
      " ['↓' '→' '→' '↓' '↓']\n",
      " ['↓' -1.0 -1.0 '↓' '↓']\n",
      " ['↓' -1.0 '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' 0.0]]\n"
     ]
    }
   ],
   "source": [
    "V, P = PolicyIteration(table.copy(), R, politique.copy(), mapping, discount=0.9, epsilon=0.01)\n",
    "print(\"Policy of POLICY ITERATION :\\n \", P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy seems to work. We can see Policy iteration is faster than Value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

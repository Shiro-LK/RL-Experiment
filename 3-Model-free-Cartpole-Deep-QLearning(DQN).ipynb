{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning (DQN) :\n",
    "\n",
    "The main idea of DQN is to use the Q-Learning algorithm with a neural network, which will approximate the action/value function. \n",
    "\n",
    "Q-learning works well when we have a simple environment, but when the number of states and actions increases, the problem become much more complex (more parameters and slower to train). So in practice, it becomes impossible to solve. That's why we try to approximate the value/action function with a neural network, instead of using lookup table.\n",
    "\n",
    "DQN is an algorithm unstable. In order to overcome that, an Experience Replay has been implemented . The goal is to save the different state, reward action and next state at each step in our memory. Then, we can subsample n \n",
    "\n",
    "$Q(s_{t}, a) \\leftarrow Q(s_{t}, a)+\\alpha(r_{t+1}+\\gamma \\max_{p} Q(s_{t+1}, p))-Q(s_{t}, a))$\n",
    "\n",
    "An improvement to reduce the unstability is to used a target network (freeze network) and a policy network (use for selecting the action during the experiment). The target network is updated every n steps (copy of the policy network)\n",
    "$r_{t+1}+\\gamma \\max_{p} Q(s_{t+1}, p)$ is computing by the target network.\n",
    "\n",
    "<img src=\"img/dqn.JPG\" >    \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from moviepy.editor import ImageSequenceClip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "original_max_ep = env._max_episode_steps\n",
    "env._max_episode_steps = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(env, policy_model, epsilon, device,gif_name=None):\n",
    "    frames = []\n",
    "    observation = env.reset()\n",
    "    total_r = 0.\n",
    "    state=np.array(observation).reshape(1,-1).astype(np.float32)\n",
    "    for curr_step in range(env._max_episode_steps):\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "        action = select_action(env, policy_model, state, epsilon=epsilon, device=device)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state = np.array(observation).reshape(1,-1).astype(np.float32)\n",
    "        if done:\n",
    "            \n",
    "            print(f\"num_steps:{curr_step} \")\n",
    "            break\n",
    "           \n",
    "    env.close()\n",
    "    if gif_name is not None:\n",
    "        clip = ImageSequenceClip(frames, fps=20)\n",
    "        clip.write_gif(gif_name, fps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, batch_size=32, max_memory=10000):\n",
    "        self.batch_size=batch_size\n",
    "        self.memory = []\n",
    "        self.max_memory = max_memory\n",
    "\n",
    "    def update(self, x):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop()\n",
    "\n",
    "        self.memory.append(x)\n",
    "        \n",
    "    def sample(self):\n",
    "        idx = np.random.randint(0, len(self.memory), (self.batch_size))\n",
    "        \n",
    "        return [self.memory[i] for i in idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self,  HIDDEN_LAYER=30):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.outputs = nn.Linear(HIDDEN_LAYER, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.outputs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, model, state, epsilon=None, device=\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        if epsilon is None:\n",
    "\n",
    "            q = model(torch.as_tensor(state).to(device)).detach().cpu().numpy()\n",
    "            return np.argmax(q, axis=-1)[0]\n",
    "        else:\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "            else:\n",
    "                q = model(torch.as_tensor(state).to(device)).detach().cpu().numpy()\n",
    "                #print(q)\n",
    "                action = np.argmax(q, axis=-1)[0]\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1.0\n",
    "counts_steps = []\n",
    "success = 0.\n",
    "numberExp = 10000\n",
    "expdecay = 5000\n",
    "discount = 0.98\n",
    "save_name = \"dqn_mlp.pth\"\n",
    "device = \"cpu\"\n",
    "loss_fn = nn.MSELoss()\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay = ReplayMemory(batch_size=64, max_memory=5000)\n",
    "policy_model = ValueNetwork().to(device)\n",
    "policy_model.train()\n",
    "target_model = ValueNetwork().to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-2, amsgrad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(policy_model, target_model, replay, optimizer, loss_fn, discount=0.95, device=\"cuda\"):\n",
    "    if len(replay.memory) < replay.batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = replay.sample()\n",
    "    state, action, reward, new_state = zip(*batch)\n",
    "    #state, action  = torch.as_tensor(np.asarray(state).reshape(-1,4)).to(device), torch.as_tensor(action).to(device)\n",
    "    #reward, new_state = torch.as_tensor(reward).to(device), torch.as_tensor(np.asarray(new_state).reshape(-1,4)).to(device)\n",
    "    state = torch.cat(state)\n",
    "    new_state = torch.cat(new_state)\n",
    "    action = torch.stack(action, dim=-1)\n",
    "    reward = torch.stack(reward, dim=-1)\n",
    "    done = (reward != -10).float() # if reward == -10, exp is finished, then the targets = reward, so we create a mask.\n",
    "    # estimate q value\n",
    "    #print(reward.shape)\n",
    "    q = policy_model(state)\n",
    "    one_hot = F.one_hot(action).bool()\n",
    "    #print(q.shape, one_hot.shape, action)\n",
    "    q = q[one_hot] # torch.gather(q, 1,  action)\n",
    "    # expected max q value in next state\n",
    "    with torch.no_grad():\n",
    "        max_q = target_model(new_state).detach().max(1)[0]\n",
    "        targets = reward + (discount * max_q*done)\n",
    "    \n",
    "    # correction\n",
    "    optimizer.zero_grad()\n",
    "    L = loss_fn(targets, q)\n",
    "    \n",
    "    L.backward()\n",
    "    #torch.nn.utils.clip_grad_norm(policy_model.parameters(), 1.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exp : 0  num_steps:13, epsilon=1.0\n",
      " exp : 200  num_steps:10, epsilon=0.9602\n",
      " exp : 400  num_steps:23, epsilon=0.9202\n",
      " exp : 600  num_steps:69, epsilon=0.8802\n",
      " exp : 800  num_steps:46, epsilon=0.8402\n",
      " exp : 1000  num_steps:32, epsilon=0.8002\n",
      " exp : 1200  num_steps:54, epsilon=0.7602\n",
      " exp : 1400  num_steps:15, epsilon=0.7202\n",
      " exp : 1600  num_steps:56, epsilon=0.6802\n",
      " exp : 1800  num_steps:13, epsilon=0.6402\n",
      " exp : 2000  num_steps:12, epsilon=0.6002\n",
      " exp : 2200  num_steps:33, epsilon=0.5602\n",
      " exp : 2400  num_steps:15, epsilon=0.5202\n",
      " exp : 2600  num_steps:102, epsilon=0.4802\n",
      " exp : 2800  num_steps:18, epsilon=0.4402\n",
      " exp : 3000  num_steps:13, epsilon=0.4002\n",
      " exp : 3200  num_steps:107, epsilon=0.3602\n",
      " exp : 3400  num_steps:14, epsilon=0.3202\n",
      " exp : 3600  num_steps:81, epsilon=0.2802\n",
      " exp : 3800  num_steps:90, epsilon=0.2402\n",
      " exp : 4000  num_steps:13, epsilon=0.2002\n",
      " exp : 4200  num_steps:53, epsilon=0.1602\n",
      " exp : 4400  num_steps:99, epsilon=0.1202\n",
      " exp : 4600  num_steps:92, epsilon=0.0802\n",
      " exp : 4800  num_steps:70, epsilon=0.0402\n",
      " exp : 5000  num_steps:47, epsilon=0.0002\n",
      " exp : 5200  num_steps:60, epsilon=0.0\n",
      " exp : 5400  num_steps:57, epsilon=0.0\n",
      " exp : 5600  num_steps:59, epsilon=0.0\n",
      " exp : 5800  num_steps:44, epsilon=0.0\n",
      " exp : 6000  num_steps:60, epsilon=0.0\n",
      " exp : 6200  num_steps:45, epsilon=0.0\n",
      " exp : 6400  num_steps:60, epsilon=0.0\n",
      " exp : 6600  num_steps:52, epsilon=0.0\n",
      " exp : 6800  num_steps:48, epsilon=0.0\n",
      " exp : 7000  num_steps:264, epsilon=0.0\n",
      " exp : 7200  num_steps:69, epsilon=0.0\n",
      " exp : 7400  num_steps:55, epsilon=0.0\n",
      " exp : 7600  num_steps:54, epsilon=0.0\n",
      " exp : 7800  num_steps:50, epsilon=0.0\n",
      " exp : 8000  num_steps:126, epsilon=0.0\n",
      " exp : 8200  num_steps:264, epsilon=0.0\n",
      " exp : 8400  num_steps:167, epsilon=0.0\n",
      " exp : 8600  num_steps:499, epsilon=0.0\n",
      "[499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 159.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 126.0, 64.0, 80.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0, 499.0]\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "R = []\n",
    "steps = 0\n",
    "leave=False\n",
    "for exp in range(numberExp+100):\n",
    "    \n",
    "    # launch experience\n",
    "    observation = env.reset()\n",
    "    total_r = 0.\n",
    "    state = np.array(observation).reshape(1,-1).astype(np.float32)\n",
    "    for curr_step in range(env._max_episode_steps+5000):\n",
    "        action = select_action(env, policy_model, state, epsilon=epsilon, device=device)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        new_state = np.array(observation).reshape(1,-1).astype(np.float32)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            reward = -10\n",
    "            \n",
    "        state, action  = torch.as_tensor(state).to(device), torch.as_tensor(action).to(device)\n",
    "        reward, new_state = torch.as_tensor(reward).to(device), torch.as_tensor(new_state).to(device)\n",
    "        replay.update([state, action, reward, new_state])\n",
    "        \n",
    "        \n",
    "            \n",
    "        if done:\n",
    "            R.append(total_r)\n",
    "            if np.mean(R[-20:]) >= best_score and epsilon == 0:\n",
    "                best_score = np.mean(R[-20:])\n",
    "                torch.save(policy_model.state_dict(), save_name)\n",
    "                if best_score == 499:\n",
    "                    print(R[-50:])\n",
    "                    print(\"finished\")\n",
    "                    leave=True\n",
    "\n",
    "            if exp%200 == 0:\n",
    "                print(f\" exp : {exp}  num_steps:{curr_step}, epsilon={epsilon}\")\n",
    "\n",
    "            counts_steps.append(curr_step)    \n",
    "            \n",
    "            break\n",
    "        total_r += reward.cpu().numpy()\n",
    "        state=new_state\n",
    "        steps += 1\n",
    "    train_step(policy_model, target_model, replay, optimizer, loss_fn, discount, device)\n",
    "    if exp % 50 == 0:\n",
    "        target_model.load_state_dict(policy_model.state_dict())\n",
    "    epsilon = max(0.00, (expdecay-exp)/expdecay)\n",
    "    if leave:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_model.load_state_dict(torch.load(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   2%|█▎                                                                  | 10/501 [00:00<00:05, 95.16it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps:499 \n",
      "MoviePy - Building file DQN_cartpole.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "test_model(env, policy_model, epsilon, device, gif_name=\"DQN_cartpole.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
